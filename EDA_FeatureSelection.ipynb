{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-2FbS_m2Gmo"
      },
      "outputs": [],
      "source": [
        "!pip install graphframes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import collect_list\n",
        "from pyspark.ml.feature import HashingTF, MinHashLSH\n",
        "from graphframes import GraphFrame"
      ],
      "metadata": {
        "id": "QXEWyvlu2JDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start Spark session with GraphFrames support\n",
        "spark = SparkSession.builder.appName(\"BigDataProject\").config(\"spark.jars.packages\", \"graphframes:graphframes:0.8.3-spark3.4-s_2.12\").getOrCreate()\n",
        "\n",
        "# GraphFrames needs a checkpoint dir\n",
        "spark.sparkContext.setCheckpointDir(\"/tmp/graphframes_checkpoint\")"
      ],
      "metadata": {
        "id": "dlGk-ISe2KVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CSV datasets\n",
        "orders = spark.read.csv(\"orders.csv\", header=True, inferSchema=True)\n",
        "order_prior = spark.read.csv(\"order_products__prior.csv\", header=True, inferSchema=True)\n",
        "order_train = spark.read.csv(\"order_products__train.csv\", header=True, inferSchema=True)\n",
        "products = spark.read.csv(\"products.csv\", header=True, inferSchema=True)\n",
        "aisles = spark.read.csv(\"aisles.csv\", header=True, inferSchema=True)\n",
        "departments = spark.read.csv(\"departments.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Save all loaded DataFrames to Parquet format for faster I/O later\n",
        "products.write.parquet(\"products.parquet\", mode=\"overwrite\")\n",
        "aisles.write.parquet(\"aisles.parquet\", mode=\"overwrite\")\n",
        "departments.write.parquet(\"departments.parquet\", mode=\"overwrite\")\n",
        "orders.write.parquet(\"orders.parquet\", mode=\"overwrite\")\n",
        "order_prior.write.parquet(\"order_products__prior.parquet\", mode=\"overwrite\")\n",
        "order_train.write.parquet(\"order_products__train.parquet\", mode=\"overwrite\")"
      ],
      "metadata": {
        "id": "A23i4V_82LzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import col, count\n",
        "\n",
        "# Check for missing values in all datasets\n",
        "for df, name in [\n",
        "    (orders, \"orders\"),\n",
        "    (order_prior, \"order_prior\"),\n",
        "    (order_train, \"order_train\"),\n",
        "    (products, \"products\"),\n",
        "    (aisles, \"aisles\"),\n",
        "    (departments, \"departments\")\n",
        "]:\n",
        "    print(f\"Missing values in {name}:\")\n",
        "    df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns]).show()\n",
        "\n",
        "# Check for fully duplicate rows in each dataset\n",
        "for df, name in [\n",
        "    (orders, \"orders\"),\n",
        "    (order_prior, \"order_prior\"),\n",
        "    (order_train, \"order_train\"),\n",
        "    (products, \"products\"),\n",
        "    (aisles, \"aisles\"),\n",
        "    (departments, \"departments\")\n",
        "]:\n",
        "    dup_count = df.groupBy(df.columns).count().filter(\"count > 1\").count()\n",
        "    print(f\"Duplicate rows in {name}: {dup_count}\")\n",
        "\n",
        "\n",
        "# There are duplicate values not shown here, such as product ID,\n",
        "# but it is normal since one product can be ordered multiple times by the same or different customer"
      ],
      "metadata": {
        "id": "Knztz-Ys2NVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Join prior orders with order metadata\n",
        "merged_prior = order_prior.join(orders, on=\"order_id\", how=\"inner\")"
      ],
      "metadata": {
        "id": "H2LlAGgU2O3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Join train orders with order metadata\n",
        "merged_train = order_train.join(orders, on=\"order_id\", how=\"inner\")"
      ],
      "metadata": {
        "id": "ZM9Ri7tU2RGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine both into one dataset (prior + train)\n",
        "stacked_df = merged_train.union(merged_prior)"
      ],
      "metadata": {
        "id": "CLoEpBwU2UUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Repartition for performance and cache for repeated use\n",
        "stacked_df = stacked_df.repartition(\"user_id\").cache()\n",
        "stacked_df.count()"
      ],
      "metadata": {
        "id": "ymRME5Ho2WVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check how many unique products we have\n",
        "stacked_df.select(\"product_id\").distinct().count()"
      ],
      "metadata": {
        "id": "o5wrA9hR2X89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check how many times they were reordered\n",
        "from pyspark.sql.functions import count\n",
        "\n",
        "most_popular_products = (\n",
        "    stacked_df.groupBy(\"product_id\").count()\n",
        "    .join(products, on=\"product_id\", how=\"inner\")\n",
        "    .sort(\"count\", ascending=False)\n",
        "    .limit(20)\n",
        ")\n",
        "\n",
        "most_popular_products.show()"
      ],
      "metadata": {
        "id": "HPcWWlp02ZeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert Spark DataFrame to Pandas\n",
        "top_products_pd = most_popular_products.toPandas()\n",
        "\n",
        "top_products_pd = top_products_pd.sort_values(\"count\", ascending=True)\n",
        "\n",
        "# Plot\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.barh(top_products_pd[\"product_name\"], top_products_pd[\"count\"], color='green')\n",
        "plt.xlabel(\"Number of Orders\")\n",
        "plt.title(\"Top 20 Most Popular Products\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xgwCLJMD2bAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the 20 least popular products\n",
        "\n",
        "least_popular_products = (\n",
        "    stacked_df.groupBy(\"product_id\").count()\n",
        "    .join(products, on=\"product_id\", how=\"inner\")\n",
        "    .sort(\"count\", ascending=True)\n",
        "    .limit(20)\n",
        ")\n",
        "\n",
        "least_popular_products.show()"
      ],
      "metadata": {
        "id": "IPyxY-Fy2dNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert Spark DataFrame to Pandas\n",
        "least_products_pd = least_popular_products.toPandas()\n",
        "\n",
        "# Sort by count (just for display order)\n",
        "least_products_pd = least_products_pd.sort_values(\"count\", ascending=True)\n",
        "\n",
        "# Plot\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.barh(least_products_pd[\"product_name\"], least_products_pd[\"count\"], color='tomato')\n",
        "plt.xlabel(\"Number of Orders\")\n",
        "plt.title(\"20 Least Popular Products\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "sbtLjur92fmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check how many orders happen per hour\n",
        "stacked_df.groupBy(\"order_hour_of_day\") \\\n",
        "    .agg(count(\"order_id\").alias(\"count\")) \\\n",
        "    .sort(\"count\", ascending=True) \\\n",
        "    .show()"
      ],
      "metadata": {
        "id": "KDWGKK4D2hjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count number of orders for each day of week and hour of day\n",
        "heatmap_df = stacked_df.groupBy(\"order_dow\", \"order_hour_of_day\") \\\n",
        "    .agg(count(\"*\").alias(\"order_count\"))\n",
        "\n",
        "heatmap_pd = heatmap_df.toPandas()\n",
        "\n",
        "# Create pivot table: rows = hour, columns = day\n",
        "heatmap_pivot = heatmap_pd.pivot(index=\"order_hour_of_day\", columns=\"order_dow\", values=\"order_count\")\n",
        "\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.heatmap(heatmap_pivot, cmap=\"YlOrRd\", cbar_kws={\"label\": \"Number of Orders\"})\n",
        "\n",
        "plt.title(\"Order Volume by Day of Week and Hour of Day\")\n",
        "plt.xlabel(\"Day of Week (0 = Sunday)\")\n",
        "plt.ylabel(\"Hour of Day\")\n",
        "plt.xticks(rotation=0)\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WBlG1NqF2jID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Join stacked_df with products to get department_id for each product\n",
        "df_departments = stacked_df.join(products, on=\"product_id\", how=\"inner\")\n",
        "\n",
        "department_popularity = df_departments.groupBy(\"department_id\") \\\n",
        "    .agg(count(\"*\").alias(\"total_orders\")) \\\n",
        "    .orderBy(\"total_orders\", ascending=False)\n",
        "\n",
        "most_popular_departments = department_popularity.join(departments, on=\"department_id\", how=\"inner\")\n",
        "most_popular_departments.show()\n",
        "\n",
        "# Convert to Pandas\n",
        "dept_pd = most_popular_departments.toPandas().sort_values(\"total_orders\", ascending=True)\n",
        "\n",
        "# Plot\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(dept_pd[\"department\"], dept_pd[\"total_orders\"], color=\"purple\")\n",
        "plt.xlabel(\"Number of Orders\")\n",
        "plt.title(\"Most Popular Departments\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5dU9hn4V2kwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To check which aisles are the most popular\n",
        "df_with_aisles = stacked_df.join(products, on=\"product_id\", how=\"inner\")\n",
        "\n",
        "aisle_popularity = df_with_aisles.groupBy(\"aisle_id\") \\\n",
        "    .agg(count(\"*\").alias(\"total_orders\")) \\\n",
        "    .orderBy(\"total_orders\", ascending=False)\n",
        "\n",
        "most_popular_aisles = aisle_popularity.join(aisles, on=\"aisle_id\", how=\"inner\")\n",
        "most_popular_aisles.orderBy(\"total_orders\", ascending=False).show()\n",
        "\n",
        "# Convert to pandas and keep the top 25 for plotting\n",
        "aisle_pd = most_popular_aisles.toPandas().sort_values(\"total_orders\", ascending=False).head(25)\n",
        "\n",
        "# Plot\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.barh(aisle_pd[\"aisle\"], aisle_pd[\"total_orders\"], color=\"teal\")\n",
        "plt.xlabel(\"Number of Orders\")\n",
        "plt.title(\"Most Popular Aisles\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_IMnMvQu2mgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top selling aisles within each department\n",
        "df_joined = stacked_df.join(products, \"product_id\", \"inner\") \\\n",
        "                      .join(aisles, \"aisle_id\", \"inner\") \\\n",
        "                      .join(departments, \"department_id\", \"inner\")\n",
        "\n",
        "# Create treemap data: total orders per (department, aisle) pair\n",
        "df_treemap = df_joined.groupBy(\"department\", \"aisle\") \\\n",
        "                      .agg(count(\"*\").alias(\"total_orders\"))\n",
        "\n",
        "treemap_pd = df_treemap.toPandas()\n",
        "\n",
        "import plotly.express as px\n",
        "\n",
        "fig = px.treemap(\n",
        "    treemap_pd,\n",
        "    path=[\"department\", \"aisle\"],\n",
        "    values=\"total_orders\",\n",
        "    color=\"total_orders\",  # Color by volume\n",
        "    color_continuous_scale=\"YlGnBu\",\n",
        "    title=\"Top-Selling Aisles Within Each Department\"\n",
        ")\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "8qU0I6dM2oTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group by reordered\n",
        "reorder_counts = stacked_df.groupBy(\"reordered\") \\\n",
        "    .agg(count(\"*\").alias(\"count\")) \\\n",
        "    .orderBy(\"reordered\")\n",
        "\n",
        "# Convert to Pandas\n",
        "reorder_pd = reorder_counts.toPandas()"
      ],
      "metadata": {
        "id": "fEeLJxkd2qXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot directly without .tolist()\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.bar(reorder_pd[\"reordered\"], reorder_pd[\"count\"], color=[\"lightcoral\", \"mediumseagreen\"])\n",
        "plt.title(\"Reordered vs Not Reordered\")\n",
        "plt.xlabel(\"Reordered Flag (0 = No, 1 = Yes)\")\n",
        "plt.ylabel(\"Number of Products\")\n",
        "plt.xticks([0, 1], [\"Not Reordered\", \"Reordered\"])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mcaesqcX2r5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import sum\n",
        "\n",
        "# Get total reorder count per product (only where reordered = 1)\n",
        "reordered_counts = stacked_df.filter(stacked_df.reordered == 1) \\\n",
        "    .groupBy(\"product_id\") \\\n",
        "    .agg(sum(\"reordered\").alias(\"reorder_count\")) \\\n",
        "    .orderBy(\"reorder_count\", ascending=False)\n",
        "\n",
        "# Add product names to reorder counts\n",
        "most_reordered_products = reordered_counts.join(products, on=\"product_id\", how=\"inner\")\n",
        "\n",
        "# Keep only the top 20 most reordered products\n",
        "top_reordered = most_reordered_products.limit(20)\n",
        "\n",
        "# Convert to pandas\n",
        "top_reordered_pd = top_reordered.toPandas().sort_values(\"reorder_count\", ascending=True)\n",
        "\n",
        "# Plot\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.barh(top_reordered_pd[\"product_name\"], top_reordered_pd[\"reorder_count\"], color=\"seagreen\")\n",
        "plt.xlabel(\"Reorder Count\")\n",
        "plt.title(\"Top 20 Most Reordered Products\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0WNognKG2tPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group by days_since_prior_order and order_dow\n",
        "grouped_orders = stacked_df.groupBy(\"days_since_prior_order\", \"order_dow\") \\\n",
        "    .agg(count(\"*\").alias(\"order_count\"))\n",
        "\n",
        "# Convert to pandas\n",
        "grouped_pd = grouped_orders.toPandas()\n",
        "\n",
        "# Pivot so days_since_prior_order becomes index, order_dow becomes columns\n",
        "pivot_df = grouped_pd.pivot(index=\"days_since_prior_order\", columns=\"order_dow\", values=\"order_count\")\n",
        "\n",
        "# Set column order to match days (0 = Sunday to 6 = Saturday)\n",
        "pivot_df = pivot_df[[0, 1, 2, 3, 4, 5, 6]]\n",
        "\n",
        "# Plot\n",
        "pivot_df.plot(\n",
        "    kind=\"bar\",\n",
        "    stacked=True,\n",
        "    figsize=(14, 6),\n",
        "    colormap=\"tab10\"\n",
        ")\n",
        "\n",
        "plt.title(\"Orders by Days Since Prior Order (stacked by Day of Week)\")\n",
        "plt.xlabel(\"Days Since Prior Order\")\n",
        "plt.ylabel(\"Number of Orders\")\n",
        "plt.legend(title=\"Day of Week (0=Sunday)\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ALjAziBI2u2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the average cart position for each product\n",
        "avg_cart_position = order_prior.groupBy(\"product_id\") \\\n",
        "    .agg(F.avg(\"add_to_cart_order\").alias(\"avg_cart_position\"))\n",
        "\n",
        "# Compute the total orders per product\n",
        "total_orders = order_prior.groupBy(\"product_id\") \\\n",
        "    .agg(F.count(\"order_id\").alias(\"total_orders\"))\n",
        "\n",
        "# Compute the total reorders\n",
        "total_reorders = order_prior.groupBy(\"product_id\").agg(\n",
        "    F.sum(\"reordered\").alias(\"total_reorders\")\n",
        ")\n",
        "\n",
        "# Join these features together into one DataFrame\n",
        "features = avg_cart_position.join(total_orders, on=\"product_id\", how=\"inner\")\n",
        "features1 = features.join(total_reorders, on=\"product_id\", how=\"inner\")\n",
        "features1.show()"
      ],
      "metadata": {
        "id": "R5NkJh3O2wu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all unique products that were reordered in the training set and assign label 1\n",
        "reordered_products = order_train.filter(\"reordered = 1\") \\\n",
        "    .select(\"product_id\").distinct() \\\n",
        "    .withColumn(\"label\", F.lit(1))\n",
        "\n",
        "# Join product features with the reordered labels, filling missing labels with 0\n",
        "labeled_df = features1.join(reordered_products, on=\"product_id\", how=\"left\") \\\n",
        "    .fillna(0, subset=[\"label\"])"
      ],
      "metadata": {
        "id": "zIiw4uTQ27yI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "product_info = products.select(\"product_id\", \"aisle_id\", \"department_id\")\n",
        "\n",
        "labeled_df = labeled_df.join(product_info, on=\"product_id\", how=\"left\")"
      ],
      "metadata": {
        "id": "HjBM5Qt829oL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Union merged prior and merged train datasets to capture all order-product details\n",
        "all_orders = merged_prior.union(merged_train)"
      ],
      "metadata": {
        "id": "DApUJBnd2_Dy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enriched_orders = all_orders.join(features, on='product_id', how='left')\n",
        "enriched_orders.show()"
      ],
      "metadata": {
        "id": "ElvTuapo3A66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Join user and order info to the enriched orders\n",
        "final_df = enriched_orders.join(orders, on='order_id', how='left')"
      ],
      "metadata": {
        "id": "uUysy5C-3CV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Join full product details to the final DataFrame\n",
        "official_df = final_df.join(products, on='product_id',how='inner')"
      ],
      "metadata": {
        "id": "v0AfFqvB3EB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "# Get the original column names (with duplicates)\n",
        "orig_names = official_df.schema.names\n",
        "counts     = Counter(orig_names)\n",
        "\n",
        "# Create a new name for each column, appending '_1', '_2', ... on duplicates\n",
        "name_counts = Counter()\n",
        "new_names   = []\n",
        "for name in orig_names:\n",
        "    idx = name_counts[name]\n",
        "    if idx == 0:\n",
        "        new_names.append(name)\n",
        "    else:\n",
        "        new_names.append(f\"{name}_{idx}\")\n",
        "    name_counts[name] += 1\n",
        "\n",
        "# Rename the Dataframe with the new unique column names\n",
        "official_df = official_df.toDF(*new_names)\n",
        "\n",
        "# Identify and drop all duplicated columns (with suffixes like _1, _2, etc.)\n",
        "to_drop = []\n",
        "for name, cnt in counts.items():\n",
        "    for i in range(1, cnt):\n",
        "        to_drop.append(f\"{name}_{i}\")\n",
        "\n",
        "official_df = official_df.drop(*to_drop)\n",
        "\n",
        "# Now official_df.columns will list each name only once\n",
        "print(official_df.columns)\n"
      ],
      "metadata": {
        "id": "dLX-mzYs3OZ9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}